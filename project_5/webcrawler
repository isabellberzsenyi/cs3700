#!/usr/bin/env python3
import http
import sys
from html.parser import HTMLParser

TARGET_DOMAIN = "http://webcrawler-site.ccs.neu.edu"
FAKEBOOK_ROOT = "/fakebook/"
SECRET_FLAG = "secret_flag"

class LinkFetcher(HTMLParser):
  def __init__(self):
    super().__init__()
    self.links = []
    self.secret_keys = []
    self.hasKey = False
    self.reset()

  def valid_link(self, link):
    # Implement this later
    return TARGET_DOMAIN in link

  def handle_starttag(self, tag, attrs):
    # find cookies in input tag
    if tag == "a":
      for name, link in attrs:
        if name == 'href':
          self.links.append(link)
    elif tag == "h2":
      if ('class', SECRET_FLAG) in attrs:
        self.hasKey = True

  def handle_data(self, data):
    if self.hasKey:
      # print(f'Found key!\n {data}')
      key = data.split("FLAG: ")[1]
      self.secret_keys.append(key)
      self.hasKey = False

# berzsenyi.i
# 74BMDIXZBUIK2EXD

def login(username, password):
  response = http.GET(f"{TARGET_DOMAIN}/accounts/login/?next=/fakebook/")
  tokens = http.getCookies(response)
  middleware_token = http.getMiddlewareToken(response)
  body = f"username={username}&password={password}&csrfmiddlewaretoken="+middleware_token+"&next=%2Ffakebook%2F"
  response = http.POST("http://webcrawler-site.ccs.neu.edu/accounts/login/", tokens, body)
  tokens = http.getCookies(response)
  print('logged in')
  return tokens

def getNewLocation(response):
  print('get new location')
  if 'Location' in response:
    split = response.split('\n')
    for x in split:
      if 'Location' in x:
        return x.replace("Location: ", "")

def parsePageResponse(tokens, path):
  request_path = f"{TARGET_DOMAIN}{path}"
  response = http.GET(request_path, tokens)
  response_status = response.split("\n")[0].split(" ")[1]
  while response_status != '200' and response_status != '302':  
    print('not 200 or 302')
    if response_status == '301':
      request_path = getNewLocation(response)
    elif (response_status == '403') or (response_status == '404'):
      return
    response = http.GET(request_path, tokens)
    response_status = response.split("\n")[0].split(" ")[1]

  linkFetch = LinkFetcher()
  linkFetch.feed(response)
  
  return {
    'links': linkFetch.links,
    'secret_keys': linkFetch.secret_keys 
  }

def is_invalid(link):
  return FAKEBOOK_ROOT not in link

def crawl(tokens, init_path):
  queue = [init_path]
  visited = []
  secret_keys = []

  while len(queue) != 0:
    current_path = queue.pop()
    if current_path in visited or is_invalid(current_path):
      continue
    else:
      visited.append(current_path)
      parsed_response = parsePageResponse(tokens, current_path)
      secret_keys.extend(parsed_response['secret_keys'])
      queue.extend(parsed_response['links'])
  return secret_keys

def main(username, password):
  tokens = login(username, password)
  secret_keys=crawl(tokens, FAKEBOOK_ROOT)
  for key in secret_keys:
    print(key)
  return secret_keys

if __name__ == "__main__":
  username = sys.argv[1]
  password = sys.argv[2]
  main(username, password)
